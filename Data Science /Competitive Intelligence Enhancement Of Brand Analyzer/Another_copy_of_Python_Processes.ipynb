{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x066EphISMry"
      },
      "source": [
        "#ALL FUNCTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFqPRuFySEfU"
      },
      "source": [
        "##ADD Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFi6sLnqoMx7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import xml.etree.ElementTree as et\n",
        "import csv\n",
        "from datetime import datetime,timedelta\n",
        "import pickle\n",
        "from io import BytesIO\n",
        "import sys\n",
        "import requests\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8amWDtXoNic"
      },
      "source": [
        "## Class to convert Scripts from dev to prod and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVfk5OfoSlEn"
      },
      "outputs": [],
      "source": [
        "class code_conversions:\n",
        "  load_sequence=['planisware_finance_data.sql',\n",
        "'gc_config.sql',\n",
        "'child_gc_config.sql',\n",
        "'child_gc_config_split.sql',\n",
        "'grand_gc_config.sql',\n",
        "'child_gc_req_set.sql',\n",
        "'defaultworkflow.sql',\n",
        "'vox_defaultworkflow.sql',\n",
        "'standard_attribute.sql',\n",
        "'unsatisfied_stakeholder_requirements.sql',\n",
        "'simulation.sql',\n",
        "'gcid_appitem_list.sql',\n",
        "'voe_non_dng.sql',\n",
        "'voe_requirements.sql',\n",
        "'vox_requirements.sql',\n",
        "'rqm.sql',\n",
        "'rqm_log.sql',\n",
        "'percent_test_plan_verified_phase2.sql',\n",
        "'test_plan_phase2_split.sql',\n",
        "'test_plan_verified_phase2_latest.sql',\n",
        "'percentage_phase2.sql',\n",
        "'percentage_phase2_split.sql',\n",
        "'percentage_phase2_latest.sql',\n",
        "'trend_cv.sql',\n",
        "'trend_cvdv.sql',\n",
        "'trend_dv.sql',\n",
        "'trend_pv.sql',\n",
        "'interface_variant_report.sql',\n",
        "'interface_variant_requirements_report.sql',\n",
        "'interface_variant_requirements_report_latest.sql',\n",
        "'icd_requirement_report.sql',\n",
        "'icd_requirement_report_latest.sql',\n",
        "'simulation_validation.sql',\n",
        "'simulation_validation_latest.sql',\n",
        "'test_script_maturity.sql',\n",
        "'test_script_maturity_latest.sql',\n",
        "'test_case_attribute.sql',\n",
        "'test_case_without_req.sql',\n",
        "'test_case_with_req.sql',\n",
        "'test_result.sql',\n",
        "'test_plan_attribute.sql',\n",
        "'test_plan_yield.sql',\n",
        "'sbd platform hardcoded.sql',\n",
        "'orphan_tcer_table.sql',\n",
        "'platform_orphan_table.sql',\n",
        "'orphan_monthly.sql',\n",
        "'project_portfolio.sql',\n",
        "'project_portfolio_summary_test_plan.sql',\n",
        "'ppm_gc_view.txt',\n",
        "'active_and_library_gc_configuration.txt',\n",
        "'voe_vox_defaultworkflow_view.txt',\n",
        "'vandv_rqm.txt',\n",
        "'simulation_tcer_view.txt',\n",
        "'labels_view.txt',\n",
        "'KPI_Query.txt',\n",
        "'KPI_Query_vox.txt',\n",
        "'null_voer_identifiers.txt',\n",
        "'interface_variant_report_summary.txt',\n",
        "'icd_requirement_report_summary.sql',\n",
        "'interface_variant_requirements_report_config_table.txt',\n",
        "'interface_variant_requirement_report_compliance.sql',\n",
        "'interface_variant_requirement_report_config.txt',\n",
        "'interface_variant_requirement_report_compliance_table.txt',\n",
        "'availabiliy_by_attribute.sql',\n",
        "'simulation_validation_latest_availability.txt',\n",
        "'availability_summary.sql',\n",
        "'vox_total_mismatch.txt',\n",
        "'test Plan CV+DV.txt',\n",
        "'test_plan_verfied_cvdv_summary.txt',\n",
        "'test_plan_verified_phase2_test_verdict_phase_validation.txt',\n",
        "'test_plan_verified_cvdv_tableview.txt',\n",
        "'orphan_tcer.sql',\n",
        "'coverage.sql',\n",
        "'fmea.txt',\n",
        "'physical_sbd.sql',\n",
        "'test_script_maturity_avoidance.sql',\n",
        "'avoidance_missing.sql',\n",
        "'Portfolio.sql',\n",
        "'platform_orphan_tcer.sql',\n",
        "'Vpd_project_metrics.sql',\n",
        "'coverage_orphan.sql',\n",
        "'product_category_platfrom_hardocoded.sql',\n",
        "'vpd_project_metrics_notorphan.sql',\n",
        "'vpd_project_metrics_notorphan_table.sql',\n",
        "'orphan_test_case.sql',\n",
        "'orphan_test_case_table.sql',\n",
        "'orphan_tcer_monthly_summary.sql',\n",
        "'vpd_monthly.sql']\n",
        "\n",
        "  def __init__(self,convert_to_env='dev',replace_converted_script=True,populate_dateparm=False,expand_bulk_load_func=False,\n",
        "               expand_table_setup_func=False,\n",
        "               unload_src='prd',unload_s3='qa',unload_add_created_on=True,unload_d_ate='',unload_dynamic_timestamp=False):\n",
        "    self.directory_sequence=[]\n",
        "    self.file_sequence=[]\n",
        "    self.convert_to_env=convert_to_env\n",
        "    self.replace_converted_script=replace_converted_script\n",
        "    self.populate_dateparm=populate_dateparm\n",
        "    self.unload_src=unload_src\n",
        "    self.unload_s3=unload_s3\n",
        "    self.unload_add_created_on=unload_add_created_on\n",
        "    self.unload_d_ate=unload_d_ate\n",
        "    self.unload_dynamic_timestamp=unload_dynamic_timestamp\n",
        "    self.dateparm=(datetime.now()- timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "    self.expand_bulk_load_func=expand_bulk_load_func\n",
        "    self.expand_table_setup_func=expand_table_setup_func\n",
        "\n",
        "\n",
        "\n",
        "  def get_sequence(self):\n",
        "    self.directory_sequence=os.listdir('.')\n",
        "    for expected_sequence in self.load_sequence:\n",
        "      expected_sequence=expected_sequence.split('.')[0]\n",
        "      for directory_sequence in self.directory_sequence:\n",
        "        directory_sequence_split=directory_sequence.split('.')[0]\n",
        "        if directory_sequence_split==expected_sequence:\n",
        "          self.file_sequence.append(directory_sequence)\n",
        "    print(self.file_sequence)\n",
        "\n",
        "  def remove_dup_files(self):\n",
        "    self.get_sequence()\n",
        "    for files in self.directory_sequence:\n",
        "      if ('.sql' in files or '.txt' in files or '.csv' in files or '.json' in files) and (not files.startswith(\".\")) and '_output' not in files:\n",
        "        os.remove(files)\n",
        "    for files in self.directory_sequence:\n",
        "      if ('.sql' in files or '.txt' in files) and (not files.startswith(\".\")) and '_output' in files:\n",
        "        os.replace(files,files.replace('_output',''))\n",
        "\n",
        "  def combine_files(self,name):\n",
        "    self.get_sequence()\n",
        "    df=pd.DataFrame()\n",
        "    for f in self.directory_sequence:\n",
        "      if f.endswith(\".csv\"):\n",
        "        df_sample=pd.read_csv(f)\n",
        "        df=df.append(df_sample)\n",
        "    name = name + '.csv'\n",
        "    if self.replace_converted_script==True:\n",
        "      self.remove_dup_files()\n",
        "    df.to_csv(name,index=False)\n",
        "\n",
        "\n",
        "  def create_exec_file(self):\n",
        "    view_name=''\n",
        "    self.get_sequence()\n",
        "    for file in self.directory_sequence:\n",
        "      if '.sql' in file or '.txt' in file:\n",
        "        with open(file, 'r') as f:\n",
        "          lines = f.readlines()\n",
        "        name=((file).replace('.txt','')).replace('.sql','')\n",
        "        out = open(name+'_output.sql','w')\n",
        "\n",
        "        for line in lines:\n",
        "          if self.convert_to_env == 'qa' or self.convert_to_env == 'prd':\n",
        "            line = line.replace('dev.data_', 'data.gpo_')\n",
        "            line = line.replace('dev.stg1_', 'etl.stg1_gpo_')\n",
        "            line = line.replace('dev.stg2_', 'etl.stg2_gpo_')\n",
        "            line = line.replace('dev.stg3_', 'etl.stg3_gpo_')\n",
        "            line = line.replace('dev.stg4_', 'etl.stg4_gpo_')\n",
        "            line = line.replace('dev.mbse_', 'gpo.mbse_')\n",
        "            line = line.replace('inserted_at','etl_timestamp')\n",
        "            line = line.replace('dev.table_setup','etl.table_setup')\n",
        "            line = line.replace('dev.bulk_load','etl.bulk_load')\n",
        "            if self.expand_bulk_load_func==True:\n",
        "              line=self.expand_bulk_load(line)\n",
        "            if self.expand_table_setup_func==True:\n",
        "              line=self.expand_table_setup(line)\n",
        "            line = line.replace('arn:aws:iam::233694013590:role/redshift-worker','arn:aws:iam::635540041123:role/redshift-worker')\n",
        "            if self.convert_to_env == 'prd':\n",
        "              line = line.replace('datalake-dev-s3-data/gpo-', 'datalake-env-s3-data/gpo-')\n",
        "              line = line.replace('datalake-qa-s3-data/gpo-', 'datalake-env-s3-data/gpo-')\n",
        "            elif self.convert_to_env == 'qa':\n",
        "              line = line.replace('datalake-dev-s3-data/gpo-', 'datalake-qa-s3-data/gpo-')\n",
        "              line = line.replace('datalake-env-s3-data/gpo-', 'datalake-qa-s3-data/gpo-')\n",
        "\n",
        "          if self.convert_to_env == 'dev':\n",
        "            line = line.replace('data.gpo_','dev.data_')\n",
        "            line = line.replace('etl.stg1_gpo_','dev.stg1_')\n",
        "            line = line.replace('etl.stg2_gpo_','dev.stg2_')\n",
        "            line = line.replace('etl.stg3_gpo_','dev.stg3_')\n",
        "            line = line.replace('etl.stg4_gpo_','dev.stg4_')\n",
        "\n",
        "\n",
        "            '''\n",
        "            if line.startswith('DROP VIEW IF EXISTS') or line.startswith('drop view if exists'):\n",
        "              view_name = line\n",
        "              view_name = line.split('exists')[1]\n",
        "              view_name = view_name.split('.')[1]\n",
        "              view_name = view_name.split(';')[0]\n",
        "            '''\n",
        "\n",
        "\n",
        "            line = line.replace('gpo.mbse_', 'dev.mbse_')\n",
        "            line = line.replace('inserted_at','etl_timestamp')\n",
        "            line = line.replace('etl.table_setup','dev.table_setup')\n",
        "            line = line.replace('etl.bulk_load','dev.bulk_load')\n",
        "            if self.expand_bulk_load_func==True:\n",
        "              line=self.expand_bulk_load(line)\n",
        "            if self.expand_table_setup_func==True:\n",
        "              line=self.expand_table_setup(line)\n",
        "            line = line.replace('datalake-qa-s3-data/gpo-','datalake-dev-s3-data/gpo-')\n",
        "            line = line.replace('datalake-env-s3-data/gpo-','datalake-dev-s3-data/gpo-')\n",
        "            line = line.replace('arn:aws:iam::635540041123:role/redshift-worker','arn:aws:iam::233694013590:role/redshift-worker')\n",
        "            '''\n",
        "            if line.lower().startswith('grant select on'):\n",
        "\n",
        "              line = line.replace('GRANT SELECT ON ALL TABLES IN SCHEMA gpo TO GROUP gpo;','')\n",
        "              line = f'grant all on dev.{view_name} to handoa1;'\n",
        "            '''\n",
        "\n",
        "\n",
        "          if self.populate_dateparm==True:\n",
        "            match = re.search(r'{dateparm}', line)\n",
        "            if match:\n",
        "              line = re.sub(r'{dateparm}', self.dateparm, line)\n",
        "          out.write(line)\n",
        "        out.close()\n",
        "    if self.replace_converted_script==True:\n",
        "      self.remove_dup_files()\n",
        "\n",
        "\n",
        "  def get_load(self,file):\n",
        "    with open(file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    load_copy=0\n",
        "    load_preprocess=0\n",
        "    load_insert=0\n",
        "    res=''\n",
        "    copy=''\n",
        "    preprocess=''\n",
        "    insert=''\n",
        "    for line in lines:\n",
        "      line=line.replace('  ',' ')\n",
        "      if (line.lower().startswith(\"truncate table etl.stg1_gpo_mbse_\") or line.lower().startswith(\"truncate table dev.stg1_mbse_\") or\n",
        "          line.lower().startswith(\"copy \") or line.startswith(\"call dev.bulk_load(\") or line.startswith(\"call etl.bulk_load(\")) and load_copy==0:\n",
        "        load_copy=1\n",
        "      if load_copy==1:\n",
        "        if \";\" in line:\n",
        "          copy+=line+\"\\n\"\n",
        "          load_copy=0\n",
        "        else:\n",
        "          if self.populate_dateparm==True:\n",
        "            match = re.search(r'{dateparm}', line)\n",
        "            if match:\n",
        "              line = re.sub(r'{dateparm}', self.dateparm, line)\n",
        "          copy+=line\n",
        "\n",
        "    for line in lines:\n",
        "      line=line.replace('  ',' ')\n",
        "      if (line.lower().startswith(\"truncate table etl.stg4_gpo_mbse_\") or line.lower().startswith(\"truncate table dev.stg4_mbse_\") or\n",
        "          line.lower().startswith(\"call dev.table_setup('dev.stg4_mbse_\") or line.lower().startswith(\"call etl.table_setup('etl.stg4_gpo_mbse_\") or\n",
        "          line.lower().startswith(\"insert into dev.stg4_mbse_\") or line.lower().startswith(\"insert into etl.stg4_gpo_mbse_\")) and load_preprocess==0:\n",
        "        load_preprocess=1\n",
        "      if load_preprocess==1:\n",
        "        if \";\" in line:\n",
        "          preprocess+=line+\"\\n\"\n",
        "          load_preprocess=0\n",
        "        else:\n",
        "          preprocess+=line\n",
        "\n",
        "    for line in lines:\n",
        "      line=line.replace('  ',' ')\n",
        "      if (line.lower().startswith(\"truncate table data.gpo_mbse_\") or line.lower().startswith(\"truncate table dev.data_mbse_\") or\n",
        "          line.lower().startswith(\"call dev.table_setup('dev.data_mbse_\") or line.lower().startswith(\"call etl.table_setup('data.gpo_mbse_\") or\n",
        "          line.lower().startswith(\"delete from\") or\n",
        "          line.lower().startswith(\"insert into dev.data_mbse_\") or line.lower().startswith(\"insert into data.gpo_mbse_\") or\n",
        "          line.lower().startswith(\"update dev.data_mbse_\") or line.lower().startswith(\"update data.gpo_mbse_\")) and load_insert==0:\n",
        "        load_insert=1\n",
        "      if load_insert==1:\n",
        "        if \";\" in line:\n",
        "          insert+=line+\"\\n\"\n",
        "          load_insert=0\n",
        "        else:\n",
        "          insert+=line\n",
        "    return copy,preprocess,insert\n",
        "\n",
        "\n",
        "  def get_ddl(self,file):\n",
        "    with open(file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "    ddl_drop_view=0\n",
        "    ddl_drop_table=0\n",
        "    ddl_table=0\n",
        "    ddl_view=0\n",
        "    res_drop_table=''\n",
        "    res_drop_view=''\n",
        "    res_table=''\n",
        "    res_view=''\n",
        "    #View Drop Statements\n",
        "    for line in lines:\n",
        "      if line.lower().startswith(\"drop view\") and ddl_drop_view==0:\n",
        "        ddl_drop_view=1\n",
        "      if ddl_drop_view==1:\n",
        "        if \";\" in line:\n",
        "          res_drop_view+=line+\"\\n\"\n",
        "          ddl_drop_view=0\n",
        "        else:\n",
        "          res_drop_view+=line\n",
        "\n",
        "  #Table Drop Statements\n",
        "    for line in lines:\n",
        "      if line.lower().startswith(\"drop table\") and ddl_drop_table==0:\n",
        "        ddl_drop_table=1\n",
        "      if ddl_drop_table==1:\n",
        "        if \";\" in line:\n",
        "          res_drop_table+=line+\"\\n\"\n",
        "          ddl_drop_table=0\n",
        "        else:\n",
        "          res_drop_table+=line\n",
        "\n",
        "    #Create Table statements\n",
        "    for line in lines:\n",
        "      if (line.lower().startswith(\"create table \") or line.lower().startswith(\"create or replace table \") or\n",
        "          line.lower().startswith(\"alter table \")) and ddl_table==0:\n",
        "        ddl_table=1\n",
        "      if ddl_table==1:\n",
        "        if \";\" in line:\n",
        "          res_table+=line+\"\\n\"\n",
        "          ddl_table=0\n",
        "        else:\n",
        "          res_table+=line\n",
        "\n",
        "  #Create View statements\n",
        "    for line in lines:\n",
        "      if (line.lower().startswith(\"create view \") or line.lower().startswith(\"create or replace view \")) and ddl_view==0:\n",
        "        ddl_view=1\n",
        "      if ddl_view==1:\n",
        "        if \";\" in line:\n",
        "          res_view+=line+\"\\n\"\n",
        "          ddl_view=0\n",
        "        else:\n",
        "          res_view+=line\n",
        "    return res_drop_view,res_drop_table,res_table,res_view\n",
        "\n",
        "\n",
        "  def get_staging_columns(self,file):\n",
        "    with open(file,'r') as f:\n",
        "      lines = f.readlines()\n",
        "    take=0\n",
        "    take1=0\n",
        "    res=''\n",
        "    table=''\n",
        "    path=''\n",
        "    for line in lines:\n",
        "\n",
        "      #Extract S3 Path\n",
        "      if \"copy\" in line or take1==1:\n",
        "        take1+=1\n",
        "      if take1==2 and line.startswith(\"from\"):\n",
        "        path=line.replace('from','')\n",
        "        take1=0\n",
        "      #Extract Table Columns\n",
        "      if \"CREATE TABLE etl.stg1\" in line:\n",
        "        table=line.replace('CREATE TABLE ','').replace(\"(\",'')\n",
        "      if \"CREATE TABLE etl.stg1\" in line or take==1:\n",
        "        take+=1\n",
        "      if line.startswith(\")\"):\n",
        "        take=0\n",
        "      if take==2:\n",
        "        res+=(line.split(' ')[0]).split('\\t')[0]+\",\"+\"\\n\"\n",
        "    res=res[:-2]\n",
        "    return res,table,path\n",
        "\n",
        "\n",
        "  def unload(self,file):\n",
        "    from datetime import date\n",
        "    date = date.today()\n",
        "    l_ines,tbl,path=self.get_staging_columns(file)\n",
        "\n",
        "    if self.unload_src=='dev':\n",
        "      tbl=tbl.replace(\"etl.stg1_gpo_mbse_\",\"dev.data_mbse_\")\n",
        "    elif self.unload_src=='qa':\n",
        "      tbl=tbl.replace(\"etl.stg1_gpo_mbse_\",\"gpo.mbse_\")\n",
        "    elif self.unload_src=='prd':\n",
        "      tbl=tbl.replace(\"etl.stg1_gpo_mbse_\",\"gpo.mbse_\")\n",
        "\n",
        "    if self.unload_s3=='dev':\n",
        "      id=233694013590\n",
        "      if self.unload_dynamic_timestamp  == False:\n",
        "        path=path[:-2].replace('env','dev')+'back_'+str(date)+\"_\"\n",
        "      else:\n",
        "        path = path[:-2].replace('env','dev')+'back_'+\"{dateparm}_\"\n",
        "    elif self.unload_s3=='qa':\n",
        "      id=635540041123\n",
        "      if self.unload_dynamic_timestamp  == False:\n",
        "        path=path[:-2].replace('env','env')+'back_'+str(date)+\"_\"\n",
        "      else:\n",
        "        path = path[:-2].replace('env','env')+'back_'+\"{dateparm}_\"\n",
        "    elif self.unload_s3=='prd':\n",
        "      id=635540041123\n",
        "      if self.unload_dynamic_timestamp  == False:\n",
        "        path=path[:-2].replace('env','env')+'back_'+str(date)+\"_\"\n",
        "      else:\n",
        "        path = path[:-2].replace('env','env')+'back_'+\"{dateparm}_\"\n",
        "\n",
        "\n",
        "    if self.unload_add_created_on==True:\n",
        "      unld='unload ('+\"'\"+\"select \"+l_ines+\" from \"+tbl+\" where created_on>''\"+self.unload_d_ate+\"'''\"\n",
        "    else:\n",
        "      unld='unload ('+\"'\"+\"select \"+l_ines+\" from \"+tbl+\"'\"\n",
        "    to_path=\") to\\n\"+path+\"'\\niam_role 'arn:aws:iam::\"+str(id)+\":role/redshift-worker' csv header parallel off;\"\n",
        "\n",
        "    if l_ines=='' or tbl=='' or path=='':\n",
        "      unload_stmnt=\"\\n\"\n",
        "      unload_stmnt_scott=\"\\n\"\n",
        "    else:\n",
        "      unload_stmnt=unld+to_path\n",
        "    return unload_stmnt\n",
        "\n",
        "  def get_load_ddl_unload(self):\n",
        "    from datetime import date\n",
        "    date = date.today()\n",
        "    arr_copy=''\n",
        "    arr_preprocess=''\n",
        "    arr_insert=''\n",
        "    res_unload=''\n",
        "    drop_view_ddl=''\n",
        "    drop_table_ddl=''\n",
        "    table_ddl=''\n",
        "    view_ddl=''\n",
        "    self.get_sequence()\n",
        "    for file in self.file_sequence:\n",
        "      if ('.sql' in file or '.txt' in file) and (not file.startswith(\".\")):\n",
        "        c,p,i = self.get_load(file)\n",
        "        arr_copy+=c+\"\\n\"\n",
        "        arr_preprocess+=p+\"\\n\"\n",
        "        arr_insert+=i+\"\\n\"\n",
        "\n",
        "        dv,dt,t,v=self.get_ddl(file)\n",
        "        drop_view_ddl+=dv+\"\\n\"\n",
        "        drop_table_ddl+=dt+\"\\n\"\n",
        "        table_ddl+=t+\"\\n\"\n",
        "        view_ddl+=v+\"\\n\"\n",
        "        res_unload+=\"\\n\"+self.unload(file)+\"\\n\"\n",
        "\n",
        "    res_load=arr_copy+\"\\n\"+arr_preprocess+\"\\n\"+arr_insert\n",
        "    res_ddl=drop_view_ddl+\"\\n\"+drop_table_ddl+\"\\n\"+table_ddl+\"\\n\"+view_ddl+\"\\n\"+\"GRANT SELECT ON ALL TABLES IN SCHEMA gpo TO GROUP gpo;\"\n",
        "    return res_load,res_ddl,res_unload\n",
        "\n",
        "  def convert_date(self,date_str):\n",
        "    dt_object=datetime.fromisoformat(date_str[:-1])\n",
        "    date = dt_object.date()\n",
        "    time = dt_object.time()\n",
        "    return date,time\n",
        "\n",
        "  def write_file(self,data,name):\n",
        "    out = open(name+'.sql','w')\n",
        "    for line in data:\n",
        "        out.write(line)\n",
        "    out.close()\n",
        "    return (name+'.sql')\n",
        "\n",
        "\n",
        "  def extract_error_report(self):\n",
        "    self.get_sequence()\n",
        "    xml_path=[]\n",
        "    csv_path=[]\n",
        "    name=[]\n",
        "    url=[]\n",
        "    error_message=[]\n",
        "    start_date=[]\n",
        "    start_time=[]\n",
        "    status=[]\n",
        "    stop_date=[]\n",
        "    stop_time=[]\n",
        "    state_machine_arn=[]\n",
        "    # Loop through all the JSON files in the directory\n",
        "    for filename in self.directory_sequence:\n",
        "      if filename.endswith(\".json\"):\n",
        "        with open(filename, \"r\") as jsonfile:\n",
        "          data = json.load(jsonfile)\n",
        "\n",
        "        for i in range(len(data)):\n",
        "          input_param=json.loads(data[i]['Input'])\n",
        "          try:\n",
        "            xml_path.append(input_param[\"xml_path\"])\n",
        "            csv_path.append(input_param[\"csv_path\"])\n",
        "            url.append(input_param[\"url\"])\n",
        "          except:\n",
        "            xml_path.append('')\n",
        "            csv_path.append('')\n",
        "            url.append('')\n",
        "\n",
        "          #Get Error message\n",
        "          try:\n",
        "            info=data[i]['Cause']\n",
        "            s_tart = info.index('\"errorMessage\":\"') + len('\"errorMessage\":\"')\n",
        "            e_nd = info.index('\"}', s_tart)\n",
        "\n",
        "            error_message.append(info[s_tart:e_nd])\n",
        "          except:\n",
        "            error_message.append('')\n",
        "\n",
        "          strt_date,strt_time=self.convert_date(data[i]['StartDate'])\n",
        "          start_date.append(strt_date)\n",
        "          start_time.append(strt_time)\n",
        "          status.append(data[i]['Status'])\n",
        "\n",
        "          stp_date,stp_time=self.convert_date(data[i]['StopDate'])\n",
        "          stop_date.append(stp_date)\n",
        "          stop_time.append(stp_time)\n",
        "          name.append(data[i][\"Name\"])\n",
        "          state_machine_arn.append(data[i]['StateMachineArn'])\n",
        "\n",
        "    df=pd.DataFrame({'xml_path':xml_path,\n",
        "                    'csv_path':csv_path,\n",
        "                    'url':url,\n",
        "                    'error_message':error_message,\n",
        "                    'start_date':start_date,\n",
        "                    'start_time':start_time,\n",
        "                    'status':status,\n",
        "                    'stop_date':stop_date,\n",
        "                    'stop_time':stop_time,\n",
        "                    'name':name,\n",
        "                    'state_machine_arn':state_machine_arn\n",
        "                    })\n",
        "    df['csv_path']=df['csv_path'].str.replace('^s3://datalake-prd-s3-data/gpo-mbse-datalake/', '', regex=True)\n",
        "    #print(df.head())\n",
        "    split_df = df['csv_path'].str.split('/', expand=True)\n",
        "    df['Report_Name']=split_df[0]\n",
        "    df['File_Name']=split_df[1]\n",
        "    df['start_date'] = df['start_date'].astype(str)\n",
        "    df['stop_date'] = df['stop_date'].astype(str)\n",
        "    df['start_time'] = df['start_time'].astype(str)\n",
        "    df['stop_time'] = df['stop_time'].astype(str)\n",
        "    df['start_datetime'] = pd.to_datetime(df['start_date'] +' '+ df['start_time'])\n",
        "    df['stop_datetime'] = pd.to_datetime(df['stop_date'] +' '+ df['stop_time'])\n",
        "    df['time_delta'] = df['stop_datetime'] - df['start_datetime']\n",
        "    df['time_delta'] = df['time_delta'].dt.total_seconds().apply(lambda x: '{:02}:{:02}:{:02}'.format(int(x // 3600), int((x % 3600) // 60), int(x % 60)))\n",
        "    df=df[['Report_Name','File_Name','status','start_date','start_time','stop_date','stop_time','time_delta']]\n",
        "    df.to_csv('error_report.csv',index=False)\n",
        "\n",
        "  def expand_bulk_load(self,txt):\n",
        "    if 'ppm/data/' in txt.lower():\n",
        "      txt=txt.lower().replace(\"call dev.bulk_load(\",\"\").replace(\"call etl.bulk_load(\",\"\").replace(\")\",\"\").replace(\";\",\"\").split(',')\n",
        "      line=\"TRUNCATE TABLE \"+str(txt[1]).strip().replace(\"'\",\"\")+\";\\n\"\n",
        "      line=line+\"copy \"+str(txt[1]).strip().replace(\"'\",\"\")\n",
        "      line=line+\"\\n from \"+txt[0]\n",
        "      line=line+\"\\n iam_role 'arn:aws:iam::233694013590:role/redshift-worker' \\n DELIMITER AS '|' \\n\"\n",
        "      line=line+\"DATEFORMAT 'auto' TIMEFORMAT 'auto'\\n\"\n",
        "      line=line+\"FORMAT csv EMPTYASNULL BLANKSASNULL ACCEPTINVCHARS TRUNCATECOLUMNS IGNOREBLANKLINES IGNOREHEADER 1 MAXERROR as 1;\"\n",
        "      print('in ppm')\n",
        "      return line\n",
        "    elif txt.lower().startswith(\"call dev.bulk_load(\") or txt.lower().startswith(\"call etl.bulk_load(\"):\n",
        "      txt=txt.lower().replace(\"call dev.bulk_load(\",\"\").replace(\"call etl.bulk_load(\",\"\").replace(\")\",\"\").replace(\";\",\"\").split(',')\n",
        "      line=\"TRUNCATE TABLE \"+str(txt[1]).strip().replace(\"'\",\"\")+\";\\n\"\n",
        "      line=line+\"copy \"+str(txt[1]).strip().replace(\"'\",\"\")\n",
        "      line=line+\"\\n from \"+txt[0]\n",
        "      line=line+\"\\n iam_role 'arn:aws:iam::635540041123:role/redshift-worker' \\n DELIMITER AS ',' \\n\"\n",
        "      line=line+\"DATEFORMAT 'auto' TIMEFORMAT 'auto'\\n\"\n",
        "      line=line+\"FORMAT csv EMPTYASNULL BLANKSASNULL ACCEPTINVCHARS TRUNCATECOLUMNS IGNOREBLANKLINES IGNOREHEADER 1 MAXERROR as 1;\"\n",
        "      return line\n",
        "    else:\n",
        "      return txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def expand_table_setup(self,txt):\n",
        "    if txt.lower().startswith(\"call dev.table_setup(\") or txt.lower().startswith(\"call etl.table_setup(\"):\n",
        "      txt=txt.lower().replace(\"call dev.table_setup(\",\"\").replace(\"call etl.table_setup(\",\"\").replace(\")\",\"\").replace(\";\",\"\")\n",
        "      line=\"TRUNCATE TABLE \"+str(txt).strip().replace(\"'\",\"\")+\";\\n\"\n",
        "      return line\n",
        "    else:\n",
        "      return txt\n",
        "\n",
        "  def find_max_created_on(self):\n",
        "    self.get_sequence()\n",
        "    ch = 'select max(created_on),count(*) as cnt, '\n",
        "    col_name=' as tbl'\n",
        "    files = os.listdir('.')\n",
        "    tbl=[]\n",
        "    for file in self.directory_sequence:\n",
        "        if ('.sql' in file or '.txt' in file) and (not file.startswith(\".\")) and '_output' not in file:\n",
        "            with open(file, 'r') as f:\n",
        "              lines = f.readlines()\n",
        "              for line in lines:\n",
        "                line=line.lower()\n",
        "                '''\n",
        "                LOAD SCRIPT EXTRACTION\n",
        "                with open(file, 'w') as f:\n",
        "                  for line in lines:\n",
        "                    if line.lower().startswith('call etl.bulk_load'):\n",
        "                      line =line.split(',')[1]\n",
        "                      line=line.replace('')\n",
        "                      ch = 'select max(created_on) from '\n",
        "                      line= ch+line+';\\n'\n",
        "                    f.write(line)\n",
        "\n",
        "                    if line.lower().startswith(\"insert into \"):\n",
        "                      line = line[12:]\n",
        "                      line = line.split(' ')[0]\n",
        "                      ch = 'select max(created_on) from '\n",
        "                      line = ch+line+';\\n'\n",
        "                    f.write(line)\n",
        "                '''\n",
        "                if line.startswith(\"drop view if exists \") or line.startswith(\"drop table if exists \") or line.startswith(\"drop view \") or line.startswith(\"drop table \"):\n",
        "                  line=line.replace(\"drop view if exists \",\"\")\n",
        "                  line=line.replace(\"drop table if exists \",\"\")\n",
        "                  line=line.replace(\"drop view \",\"\")\n",
        "                  line=line.replace(\"drop table \",\"\")\n",
        "                  line=line.replace(\";\",\"\").strip()\n",
        "                  if line not in tbl:\n",
        "                    if self.convert_to_env=='dev':\n",
        "                      tbl.append(line)\n",
        "                    else:\n",
        "                      if line.startswith(\"gpo.mbse_\") or line.startswith(\"dev.mbse_\"):\n",
        "                        tbl.append(line)\n",
        "    txt=''\n",
        "    for i in range(len(tbl)):\n",
        "      name=(tbl[i].split('.')[1])\n",
        "      txt=txt+ch+\"'\"+name+\"'\"+col_name+\" from \"+tbl[i]\n",
        "      if i<len(tbl)-1:\n",
        "        txt=txt+\"\\n\"+\"union \\n\"\n",
        "      else:\n",
        "        txt=txt+\";\"\n",
        "    return txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdGeUCc_ofK8"
      },
      "source": [
        "## Auxillary Functions to support XML to CSV class scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2dnf5fuoaRA"
      },
      "outputs": [],
      "source": [
        "def get_exact_field(x):\n",
        "  res='' if x is None else x\n",
        "  res=res.replace('%3A',':')\n",
        "  res=res.replace('%2F','/')\n",
        "  res=res.replace('%20',' ')\n",
        "  result=res[:900]\n",
        "  return result\n",
        "\n",
        "def get_gc_platform(sym):\n",
        "  return {\n",
        "    'GC Golden Sample PD Cooking': 'PDx',\n",
        "    'GC PD Cooking': 'Cooking',\n",
        "    'GC PD Cross Category': 'Cross Category',\n",
        "    'GC PD Dishwasher': 'Dishwasher',\n",
        "    'GC PD KASA': 'KASA',\n",
        "    'GC PD Laundry': 'Laundry',\n",
        "    'GC PD Refrigeration': 'Refrigeration'}.get(sym, ' ')\n",
        "\n",
        "def extract_from_primary_text(x):\n",
        "  res =  re.sub(re.compile(r'<.*?>'), '', x)\n",
        "  res = res.replace('&lt;', '<')\n",
        "  res = res.replace('&gt;', '>')\n",
        "  res = res.replace('&amp;','&')\n",
        "  #res= re.sub(r'&[a-zA-Z]+?;', '', x)\n",
        "  return res\n",
        "\n",
        "def prefix_rqm_verification(x):\n",
        "  return 'Verification_'+str(x)\n",
        "\n",
        "def prefix_rqm_validation(x):\n",
        "  return 'Verification_'+str(x)\n",
        "\n",
        "\n",
        "def get_interface_platform(sym):\n",
        "  return {\n",
        "      'Cooking Product Internal Interfaces':'Cooking',\n",
        "      'Cooking Product Internal System Interfaces':'Cooking',\n",
        "      'Cooking Product External System Interfaces':'Cooking',\n",
        "      'Cooking Product User Interfaces': 'Cooking',\n",
        "      'Dishwasher Product Internal System Interfaces':'Dishwasher',\n",
        "      'Dishwasher Product External System Interfaces':'Dishwasher',\n",
        "      'Dishwasher Product User Interfaces': 'Dishwasher',\n",
        "      'Interface - [Dryer] Product Internal Interfaces':'Dryers',\n",
        "      'Clothes Washer Product User Interfaces':'Laundry',\n",
        "      'Interface - [Laundry] Product External System Interfaces':'Laundry',\n",
        "      'Interface - [HA Washer] Product Internal System Interfaces':'Laundry - HA',\n",
        "      '[VA Washer] Interface - Product Internal System Interfaces' :'Laundry - VA',\n",
        "      'Refrigerator Product Internal System Interfaces':'Refrigeration',\n",
        "      'Refrigerator Product User Interfaces':'Refrigeration',\n",
        "      'Refrigerator Product External System Interfaces':'Refrigeration',\n",
        "      'Appliance External System Interfaces':'Cross'}.get(sym, ' ')\n",
        "\n",
        "def extract_gcid(xml_path, schema_path):\n",
        "  xml=xml_path.split('/')[-1]\n",
        "  schema=schema_path.split('/')[-1]\n",
        "  return ''.join(set(xml.replace('.xml',''))-set(schema.replace('.pkl','')))\n",
        "\n",
        "def get_interface_type(sym):\n",
        "  return {\n",
        "      'Cooking Product External System Interfaces':'External',\n",
        "      'Dishwasher Product External System Interfaces':'External',\n",
        "      'Interface - [Laundry] Product External System Interfaces':'External',\n",
        "      '[VA Washer] Interface - Product Internal System Interfaces':'Internal',\n",
        "      'Cooking Product Internal Interfaces':'Internal',\n",
        "      'Cooking Product Internal System Interfaces':'Internal',\n",
        "      'Dishwasher Product Internal System Interfaces':'Internal',\n",
        "      'Interface - [Dryer] Product Internal Interfaces':'Internal',\n",
        "      'Interface - [HA Washer] Product Internal System Interfaces':'Internal',\n",
        "      'Refrigerator Product Internal System Interfaces':'Internal',\n",
        "      'Cooking Product User Interfaces':'User',\n",
        "      'Dishwasher Product User Interfaces':'User',\n",
        "      'Refrigerator Product User Interfaces':'User',\n",
        "      'Clothes Washer Product User Interfaces':'User',\n",
        "      'Refrigerator Product External System Interfaces':'External'}.get(sym, ' ')\n",
        "\n",
        "\n",
        "def get_gcid_split(x):\n",
        "  return '' if x is None else x.split('%2F')[-1]\n",
        "\n",
        "\n",
        "def get_created_on(x):\n",
        "    #return datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return (datetime.today() + timedelta(hours=5,minutes=30)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "def get_accesskey_gc_config(x):\n",
        "    return 'EESCIBDW'\n",
        "\n",
        "\n",
        "def get_accessvalue_gc_config(x):\n",
        "    return 'Y2c0YlIzZHY'\n",
        "\n",
        "\n",
        "def get_voe_req_type(sym):\n",
        "  sym=sym.split('}')[-1]\n",
        "  for key in {\n",
        "      'dng_merged_VoERequirement1_identifier':'VoE',\n",
        "      'dng_merged_PDComponentRequirement1_identifier':'PD Component',\n",
        "      'dng_merged_PDModuleRequirement1_identifier':'PD Module',\n",
        "      'dng_merged_PDSubsystemRequirement1_identifier':'PD Subsystem'\n",
        "  }.keys():\n",
        "      if sym in key:\n",
        "          return {\n",
        "              'dng_merged_VoERequirement1_identifier':'VoE',\n",
        "              'dng_merged_PDComponentRequirement1_identifier':'PD Component',\n",
        "              'dng_merged_PDModuleRequirement1_identifier':'PD Module',\n",
        "              'dng_merged_PDSubsystemRequirement1_identifier':'PD Subsystem'\n",
        "          }[key]\n",
        "  return ''\n",
        "\n",
        "\n",
        "\n",
        "def get_vox_req_type(sym):\n",
        "  sym=sym.split('}')[-1]\n",
        "  for key in {\n",
        "      'dng_merged_ConsumerStakeholderNeed1_identifier':'Consumer Stakeholder Need',\n",
        "      'dng_merged_GeneralStakeholderNeed1_identifier':'General Stakeholder Need',\n",
        "      'dng_merged_StakeholderRequirement1_identifier':'Stakeholder Requirement'\n",
        "  }.keys():\n",
        "      if sym in key:\n",
        "          return {\n",
        "              'dng_merged_ConsumerStakeholderNeed1_identifier':'Consumer Stakeholder Need',\n",
        "              'dng_merged_GeneralStakeholderNeed1_identifier':'General Stakeholder Need',\n",
        "              'dng_merged_StakeholderRequirement1_identifier':'Stakeholder Requirement'\n",
        "          }[key]\n",
        "  return ''\n",
        "\n",
        "\n",
        "def get_blank_values(x):\n",
        "  return ''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYTle_JoAcJ"
      },
      "source": [
        "## Class to extract any xml to csv with or without schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq1cvPXjn---"
      },
      "outputs": [],
      "source": [
        "class rational_to_csv:\n",
        "  def __init__(self,url,filename,schema_dictionary='',process_gcid='',populate_column=False,column_to_populate=''):\n",
        "    self.d_ict=schema_dictionary\n",
        "    self.gcid=process_gcid\n",
        "    if process_gcid=='':\n",
        "      self.url=url\n",
        "    else:\n",
        "      self.url=url.format(process_gcid)\n",
        "    self.pattern=populate_column\n",
        "    self.filename=filename\n",
        "    self.colname=column_to_populate\n",
        "\n",
        "  def get_headers(self,xml_schema):\n",
        "    headers=[]\n",
        "    for val in xml_schema.values():\n",
        "      if isinstance(val,list):\n",
        "        for ele in val:\n",
        "          if isinstance(ele,tuple):\n",
        "            if \"_get_tag\" in ele[0]:\n",
        "              headers.append(ele[0].replace('_get_tag',''))\n",
        "            else:\n",
        "              headers.append(ele[0])\n",
        "          else:\n",
        "            if \"_get_tag\" in ele:\n",
        "              headers.append(ele.replace('_get_tag',''))\n",
        "            else:\n",
        "              headers.append(ele)\n",
        "      elif isinstance(val,tuple):\n",
        "        if \"_get_tag\" in val[0]:\n",
        "          headers.append(val[0].replace('_get_tag',''))\n",
        "        else:\n",
        "          headers.append(val[0])\n",
        "      else:\n",
        "        headers.append(val)\n",
        "    return headers\n",
        "\n",
        "\n",
        "  def append_subset_xml(self,node,tagvalue):\n",
        "    if isinstance(tagvalue, tuple):\n",
        "      new_tag_name, mapper = tagvalue\n",
        "      if \"get_tag\" in new_tag_name:\n",
        "        new_tag_name=new_tag_name.replace('_get_tag','')\n",
        "        new_child = et.Element(new_tag_name)\n",
        "        new_child.text = str(mapper(node.tag)) if callable(mapper) else node.tag\n",
        "      else:\n",
        "        new_child = et.Element(new_tag_name)\n",
        "        new_child.text = str(mapper(node.text)) if callable(mapper) else node.text\n",
        "    else:\n",
        "      new_child = et.Element(tagvalue)\n",
        "      new_child.text = node.text\n",
        "    return new_child\n",
        "\n",
        "\n",
        "  def create_pkl(self):\n",
        "    self.xml_schema=pickle.dump(self.d_ict)\n",
        "    return self.xml_schema\n",
        "\n",
        "\n",
        "\n",
        "  def write_file(self,node):\n",
        "    headers = self.get_headers(self.d_ict)\n",
        "    processed_rows = set()\n",
        "    rows_to_write = []\n",
        "    for child in node:\n",
        "      row = [child.find(tag_name).text if child.find(tag_name) is not None else '' for tag_name in headers]\n",
        "      if tuple(row) not in processed_rows:\n",
        "        rows_to_write.append(row)\n",
        "        processed_rows.add(tuple(row))\n",
        "    df = pd.DataFrame(rows_to_write, columns=headers)\n",
        "\n",
        "    if not df.empty:\n",
        "      if not self.pattern==True:\n",
        "        df[self.colname] = self.gcid\n",
        "    df.to_csv(self.filename+\".csv\",index=False)\n",
        "\n",
        "\n",
        "  def xml_processing(self):\n",
        "    if self.d_ict=='':\n",
        "      xml_data=(requests.get(self.url, auth = ('EESCIBDW','cg4bR3dv'))).text\n",
        "      df = pd.read_xml(xml_data)\n",
        "      df.to_csv(self.filename+\".csv\",index=False)\n",
        "    else:\n",
        "      xml_data=(requests.get(self.url, auth = ('EESCIBDW','cg4bR3dv'))).content\n",
        "      #xml_schema=self.create_pkl()\n",
        "      xml_schema=self.d_ict\n",
        "      root = et.fromstring(xml_data)\n",
        "      subset_root = et.Element('root')\n",
        "      for child in root:\n",
        "        new_result = et.Element('root')\n",
        "        for sub_child in child:\n",
        "          tag = sub_child.tag.split('}')[1]\n",
        "          for item in xml_schema:\n",
        "            if isinstance(item,tuple):\n",
        "              for t in item:\n",
        "                if tag.startswith(t) and not tag.startswith(t+\"_\") and sub_child.text is not None:\n",
        "                  tag_value = next(xml_schema[key] for key in xml_schema.keys() if t in key)\n",
        "                  if isinstance(tag_value,list):\n",
        "                    for ele in tag_value:\n",
        "                      new_result.append(self.append_subset_xml(sub_child,ele))\n",
        "                      if len(new_result) > 0:\n",
        "                        subset_root.append(new_result)\n",
        "                  else:\n",
        "                    new_result.append(self.append_subset_xml(sub_child,tag_value))\n",
        "                    if len(new_result) > 0:\n",
        "                      subset_root.append(new_result)\n",
        "            else:\n",
        "              if tag.startswith(item) and not tag.startswith(item+\"_\"):\n",
        "                tag_value = xml_schema[item]\n",
        "                if isinstance(tag_value,list):\n",
        "                  for ele in tag_value:\n",
        "                    new_result.append(self.append_subset_xml(sub_child,ele))\n",
        "                    if len(new_result) > 0:\n",
        "                      subset_root.append(new_result)\n",
        "                else:\n",
        "                  new_result.append(self.append_subset_xml(sub_child,tag_value))\n",
        "                  if len(new_result) > 0:\n",
        "                    subset_root.append(new_result)\n",
        "\n",
        "\n",
        "      if not subset_root:\n",
        "          print('no data in xml')\n",
        "          return\n",
        "\n",
        "      self.write_file(subset_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJEsmF5eo1Gj"
      },
      "source": [
        "# Class & Script Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0vlrs_hkyTI",
        "outputId": "51d59aa0-a56c-4fab-a628-189207b62515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "sriyash=code_conversions(convert_to_env='prd',replace_converted_script=True,populate_dateparm=False,expand_bulk_load_func=False,\n",
        "                        expand_table_setup_func=False,\n",
        "                         unload_src='prd',unload_s3='prd',unload_add_created_on=True,\n",
        "                         unload_d_ate='',unload_dynamic_timestamp=False)\n",
        "\n",
        "#_,ddl,_=sriyash.get_load_ddl_unload()\n",
        "#sriyash.write_file(load,'load_gpo_new')\n",
        "#tst=sriyash.find_max_created_on()\n",
        "#sriyash.write_file(ddl,'additional_ddl_REQ1987761')\n",
        "#sriyash.remove_dup_files()\n",
        "sriyash.create_exec_file()\n",
        "#sriyash.combine_files(\"simulation\")\n",
        "#sriyash.extract_error_report()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJKeqYeDavfa"
      },
      "outputs": [],
      "source": [
        "url_voe_new='https://jrs.whirlpool.oncloudone.com/rs/query/25976/dataservice?report=25957&limit=-1&oslc_config.context=https%3A%2F%2Frelm.whirlpool.oncloudone.com%2Fgc%2Fconfiguration%2F1529&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=bcbb52b6b49679c0512891969de42147&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=1cf42e38d53ea40eb4d8ce5ad50a5cb3&pName=1cf42e38d53ea40eb4d8ce5ad50a5cb3&pName=1cf42e38d53ea40eb4d8ce5ad50a5cb3&pName=c3a82e3ea637c92ea3851464ac3538c3&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=cb56864e379797820f57b21e32257928&pName=50f6e2629af1dfee87a70bb43d7b4953&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_PO7oQPQDEemgmMjgllbi-Q&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_TjmcUOwJEemgmMjgllbi-Q&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_ZEUBAHP_EeqKquSH-KPQfw&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_YgU4UJKhEemiHrXVdsPwwg&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_CFjdcIrZEeujvIKcmbFanA&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_Xm1ZkLMkEemSHLJbm-CIRQ&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_VvN_4M8sEemNlL0kGqqbtQ&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_BK2Z0JaKEempLNJRu3K9Ug&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_fgWXYAUREeqgmMjgllbi-Q&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F__EPRYAUQEeqgmMjgllbi-Q&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_zAXIcAUTEeqhSMKisZXOXg&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_-J4SAPOMEei6VdHimkJ1ZA&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_MIQeINEWEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_Q-7wcP_vEemhSMKisZXOXg&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_xrnEENTiEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_4psBwNToEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_jgTF0dTrEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_Y9enUNTrEeefatrBpRDqyw&pVal=http%3A%2F%2Fjazz.net%2Fns%2Flqe%2Fmerge%2Fgensym%2Frm%2FVoE%2520Requirement%2520Set&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.approved&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.new&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23DefaultWorkflow.state.s1&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.complete&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.underreview&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FConceptValidation&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FDesignValidation&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FProductValidation&pVal=http%3A%2F%2Fjazz.net%2Fxmlns%2Fprod%2Fjazz%2Frqm%2Fprocess%2F1.0%2FWorkflowState%2Fcom.ibm.rqm.planning.common.new&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FDraft&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FSubmitted&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FInProgress&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FTestCompleted&pVal=*&pVal=*&basicAuthenticationEnabled=true'\n",
        "\n",
        "url_voe_old='https://jrs.whirlpool.oncloudone.com/rs/query/24671/dataservice?report=24652&limit=-1&oslc_config.context=https%3A%2F%2Frelm.whirlpool.oncloudone.com%2Fgc%2Fconfiguration%2F1529&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=ProjectIds&pName=bcbb52b6b49679c0512891969de42147&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=a543f326cc98f206e7ff15d75bb0544f&pName=1cf42e38d53ea40eb4d8ce5ad50a5cb3&pName=1cf42e38d53ea40eb4d8ce5ad50a5cb3&pName=1cf42e38d53ea40eb4d8ce5ad50a5cb3&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=76cbbb0d4698babfe646ae0691fb71e0&pName=cb56864e379797820f57b21e32257928&pName=50f6e2629af1dfee87a70bb43d7b4953&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_PO7oQPQDEemgmMjgllbi-Q&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_TjmcUOwJEemgmMjgllbi-Q&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_ZEUBAHP_EeqKquSH-KPQfw&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_YgU4UJKhEemiHrXVdsPwwg&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_CFjdcIrZEeujvIKcmbFanA&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_Xm1ZkLMkEemSHLJbm-CIRQ&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_VvN_4M8sEemNlL0kGqqbtQ&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_BK2Z0JaKEempLNJRu3K9Ug&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F_fgWXYAUREeqgmMjgllbi-Q&pVal=https%3A%2F%2Frm2.whirlpool.oncloudone.com%2Frm2%2Fprocess%2Fproject-areas%2F__EPRYAUQEeqgmMjgllbi-Q&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_zAXIcAUTEeqhSMKisZXOXg&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_-J4SAPOMEei6VdHimkJ1ZA&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_MIQeINEWEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_Q-7wcP_vEemhSMKisZXOXg&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_xrnEENTiEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_4psBwNToEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_jgTF0dTrEeefatrBpRDqyw&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2Fqm%2Fprocess%2Fproject-areas%2F_Y9enUNTrEeefatrBpRDqyw&pVal=http%3A%2F%2Fjazz.net%2Fns%2Flqe%2Fmerge%2Fgensym%2Frm%2FVoE%2520Requirement%2520Set&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.approved&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.new&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23DefaultWorkflow.state.s1&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.complete&pVal=http%3A%2F%2Fwww.ibm.com%2Fxmlns%2Frdm%2Fworkflow%2FDefaultWorkflow%23com.ibm.rdm.workflow.common.underreview&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FConceptValidation&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FDesignValidation&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FProductValidation&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FDraft&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FSubmitted&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FInProgress&pVal=https%3A%2F%2Fqm.whirlpool.oncloudone.com%2FCategoryValue%2FTestCompleted&pVal=*&pVal=*&basicAuthenticationEnabled=true'\n",
        "\n",
        "#url=[url0,url1,url2]\n",
        "name=['url0','url1','url2']\n",
        "\n",
        "\n",
        "#for i in range(len(url)):\n",
        "sriyash=rational_to_csv(url=url_voe_new,filename='url_voe_new',\n",
        "                      schema_dictionary='',process_gcid='',\n",
        "                      populate_column=False,column_to_populate='')\n",
        "sriyash.xml_processing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di3eCgZYmCjD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7b951b44-eb2b-4a0d-ba9e-15cee97cdcb8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Error.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b294e5b6af18>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Error.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "df=pd.read_csv(\"Error.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RONjzulKv4nC"
      },
      "outputs": [],
      "source": [
        "def get_attributes_connexus(strng):\n",
        "  cleaned=strng.replace(\"[\",'').replace(\"]\",'').replace('{','').replace('}','').replace('\"S\":','')\n",
        "  cleaned=cleaned.replace('\"M\":','').replace('\"','').split(',')\n",
        "  #Get all the attribute list\n",
        "  attr_list=[]\n",
        "  for ele in cleaned:\n",
        "    if ele.split(\":\")[0] not in attr_list:\n",
        "      attr_list.append(ele.split(\":\")[0])\n",
        "\n",
        "  #Get all parameters in attrName\n",
        "  param_list=[]\n",
        "  max_list=[]\n",
        "  min_list=[]\n",
        "  value_list=[]\n",
        "  for ele in cleaned:\n",
        "    if ele.split(\":\")[0] == 'attrName' and ele.split(\":\")[1] not in param_list:\n",
        "      param_list.append(ele.split(\":\")[1])\n",
        "\n",
        "  #Get all max_alert in attrName\n",
        "    if ele.split(\":\")[0] == 'attrMaxAlertRange' and ele.split(\":\")[1] not in param_list:\n",
        "      max_list.append(float(ele.split(\":\")[1]))\n",
        "\n",
        "  #Get all min_alert in attrName\n",
        "    if ele.split(\":\")[0] == 'attrMinAlertRange' and ele.split(\":\")[1] not in param_list:\n",
        "      min_list.append(float(ele.split(\":\")[1]))\n",
        "\n",
        "  #Get all value in attrName\n",
        "    if ele.split(\":\")[0] == 'attrValue' and ele.split(\":\")[1] not in param_list:\n",
        "      value_list.append(float(ele.split(\":\")[1]))\n",
        "\n",
        "  print(param_list)\n",
        "  col_names=[]\n",
        "  final={}\n",
        "  print(value_list)\n",
        "  for i in range(len(param_list)):\n",
        "    col_names.append(param_list[i])\n",
        "    col_names.append(param_list[i]+\"_exceeding_range\")\n",
        "\n",
        "    final[param_list[i]]=[value_list[i]]\n",
        "    val=value_list[i]\n",
        "    min_val=min_list[i]\n",
        "    max_val=max_list[i]\n",
        "    if val>=min_val and val<=max_val:\n",
        "      final[param_list[i]+\"_status\"]=[0]\n",
        "    else:\n",
        "      final[param_list[i]+\"_status\"]=[1]\n",
        "\n",
        "  final=pd.DataFrame(final)\n",
        "  return final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_rows_connexus(df):\n",
        "  fin_df=pd.DataFrame()\n",
        "  for i in range(df.shape[0]):\n",
        "    fin_df=pd.concat([fin_df,get_attributes_connexus(df.loc[i,\"attributeInfo\"])])\n",
        "  df_new=pd.concat([df.reset_index(drop=True),fin_df.reset_index(drop=True)],axis=1)\n",
        "  df_new.drop(columns=['PK','SK','attributeInfo','batchEndTime','batchId',\n",
        "                   'batchStartTime','createdAt','machineName','orgName',\n",
        "                   'plantName','processName','totalCycleDuration'],inplace=True,axis=1)\n",
        "  b = [0,10,18,24]\n",
        "  l = ['Morning','Evening','Night']\n",
        "  df_new['timeLog']=pd.to_datetime(df_new['timeLog']).dt.hour\n",
        "  df_new['timeLog'] = pd.cut(df_new['timeLog'], bins=b, labels=l, include_lowest=True)\n",
        "  df_new.to_csv(\"Final.csv\",index=False)\n",
        "  return df_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QVvmQQtJRdF"
      },
      "outputs": [],
      "source": [
        "def get_attributes_gallus(strng):\n",
        "  try:\n",
        "    cleaned=strng.replace(\"[\",'').replace(\"]\",'').replace('{','').replace('}','').replace('\"S\":','')\n",
        "    cleaned=cleaned.replace('\"M\":','').replace('\"','').split(',')\n",
        "    #Get all the attribute list\n",
        "    attr_list=[]\n",
        "    for ele in cleaned:\n",
        "      if ele.split(\":\")[0] not in attr_list:\n",
        "        attr_list.append(ele.split(\":\")[0])\n",
        "\n",
        "    #Get all parameters in attrName\n",
        "    param_list=[]\n",
        "    value_list=[]\n",
        "    for ele in cleaned:\n",
        "      if ele.split(\":\")[0] == 'attrName' and ele.split(\":\")[1] not in param_list:\n",
        "        param_list.append(ele.split(\":\")[1])\n",
        "\n",
        "    #Get all value in attrName\n",
        "      if ele.split(\":\")[0] == 'attrValue' and ele.split(\":\")[1] not in param_list:\n",
        "        try:\n",
        "          value_list.append(float(ele.split(\":\")[1]))\n",
        "        except:\n",
        "          value_list.append(ele.split(\":\")[1])\n",
        "\n",
        "    col_names=[]\n",
        "    final={}\n",
        "    for i in range(len(param_list)):\n",
        "      col_names.append(param_list[i])\n",
        "      final[param_list[i]]=[value_list[i]]\n",
        "      final=pd.DataFrame(final)\n",
        "    return cleaned\n",
        "  except:\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def add_rows_gallus(df):\n",
        "  fin_df=pd.DataFrame()\n",
        "  for i in range(df.shape[0]):\n",
        "    fin_df=pd.concat([fin_df,get_attributes_gallus(df.loc[i,\"attributeInfo\"])])\n",
        "  df_new=pd.concat([df.reset_index(drop=True),fin_df.reset_index(drop=True)],axis=1)\n",
        "  df_new.drop(columns=['PK','SK','attributeInfo','batchEndTime','batchId',\n",
        "                   'batchStartTime','createdAt','emergencySW','equipmentNumber','loggedinUser','orgName',\n",
        "                   'processName','refreshRate','totalCycleDuration'],inplace=True,axis=1)\n",
        "  # b = [0,10,18,24]\n",
        "  # l = ['Morning','Evening','Night']\n",
        "  # df_new['timeLog']=pd.to_datetime(df_new['timeLog']).dt.hour\n",
        "  # df_new['timeLog'] = pd.cut(df_new['timeLog'], bins=b, labels=l, include_lowest=True)\n",
        "  df_new.to_csv(\"Final.csv\",index=False)\n",
        "  return df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G6TxkPHfMBw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "35278e89-b977-4d20-e940-a1f9ccb3ea70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e88e2cd2d86e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml_ist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_attributes_gallus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'allErrors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "l_ist=get_attributes_gallus(df.loc[0,'allErrors'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "2SphsxP6vHyB",
        "outputId": "16d5f192-3d91-4ba0-ee3a-7d0d86a97498"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'l_ist' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e9dc38fd0208>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml_ist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'l_ist' is not defined"
          ]
        }
      ],
      "source": [
        "l_ist[0].split(':',1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "04z_IIBYwfWh",
        "outputId": "3107d535-23fd-4b90-922f-c85483452558"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'l_ist' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ada0106dbc10>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m':'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_ist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m911\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'l_ist' is not defined"
          ]
        }
      ],
      "source": [
        "':' in l_ist[911]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuCAyYsJuziH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "a2826619-ddec-45f0-d136-17af58cce9c6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'l_ist' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4e5dfbf8000b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_ist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_ist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_ist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'l_ist' is not defined"
          ]
        }
      ],
      "source": [
        "df={\n",
        "  'timeOn':[],\n",
        "  'errorText':[],\n",
        "  'errorNo':[],\n",
        "  'moduleType':[],\n",
        "  'module':[],\n",
        "  'constructionUnit':[],\n",
        "  'category':[],\n",
        "  'timeOff':[]\n",
        "}\n",
        "\n",
        "for i in range(len(l_ist)):\n",
        "  if ':' in l_ist[i]:\n",
        "    df[l_ist[i].split(':',1)[0]].append(l_ist[i].split(':',1)[1])\n",
        "  else:\n",
        "    df['timeOff'].append('2023-09-11T00:25:15.576+02:00')\n",
        "\n",
        "len(df['timeOn'])\n",
        "len(df['errorText'])\n",
        "len(df['errorNo'])\n",
        "len(df['moduleType'])\n",
        "len(df['module'])\n",
        "len(df['constructionUnit'])\n",
        "len(df['category'])\n",
        "len(df['timeOff'])\n",
        "pd.DataFrame(df).to_csv(\"Error_Report.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIgf1dRNFi41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1188530a-d985-40de-8c32-74c3e0ab6248"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e7f097161a15>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madd_rows_gallus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-eee894f634d9>\u001b[0m in \u001b[0;36madd_rows_gallus\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_rows_gallus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mfin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mfin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_attributes_gallus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"attributeInfo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mdf_new\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfin_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "add_rows_gallus(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR0fkMKDN8HW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "c04c2596-abea-4674-c030-3301791394e4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Final.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3df5cfc0653c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mana_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mana_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Final.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "ana_df=pd.read_csv(\"Final.csv\")\n",
        "ana_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab9Dv33rNq0S"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import tree\n",
        "\n",
        "df_dumm = ana_df.drop(columns=[\"timeLog\",\n",
        "                \"Reactor Temperature_status\",\"Reactor Pressure_status\",\n",
        "                \"Seperator Temperature_status\",\"Seperator Pressure_status\",\n",
        "                \"Heating Temperature_status\",\"Oee_status\",\"Mtbf_status\",\n",
        "                \"Mttr_status\",\"Machine in breakdown_status\"],axis=1)\n",
        "Y=df_dumm['emergencySW']\n",
        "X=df_dumm.drop(columns=\"emergencySW\",axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar1A-2IgOgZe"
      },
      "outputs": [],
      "source": [
        "clf = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCTAt3YJQpq_"
      },
      "outputs": [],
      "source": [
        "classification_report(y_test, clf.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBMLHeE6RJRP"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(25,20))\n",
        "_ = tree.plot_tree(clf,\n",
        "                   feature_names=X.columns,\n",
        "                   class_names=[\"0\",\"1\"],\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztlKkKV8TdNZ"
      },
      "outputs": [],
      "source": [
        "fig.savefig(\"decistion_tree.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxv6rA2wvtbA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('Final_1.csv')\n",
        "\n",
        "# Convert the 'Date' column to a datetime object if it's not already\n",
        "df['timeLog'] = pd.to_datetime(df['timeLog'])\n",
        "\n",
        "# Define the date range from 26th Aug 2023 to 25th Sep 2023\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the date range from 26th Aug 2023 to 25th Sep 2023\n",
        "start_date = datetime(2023, 8, 26)\n",
        "end_date = datetime(2023, 9, 25)\n",
        "\n",
        "# Initialize counters and other parameters\n",
        "job_counter = 2000\n",
        "partial_job_counter = random.randint(500, 2500)\n",
        "production_counter = 15000\n",
        "total_counter = 557783\n",
        "\n",
        "# Create lists to store the extrapolated data\n",
        "extrapolated_data = {\n",
        "    'machineName':[],\n",
        "    'machineStatus':[],\n",
        "    'machineType':[],\n",
        "    'plantName':[],\n",
        "    'smartbearStatus':[],\n",
        "    'Date': [],\n",
        "    'Job Counter': [],\n",
        "    'Partial Job Counter': [],\n",
        "    'Production Counter': [],\n",
        "    'Total Counter': [],\n",
        "}\n",
        "\n",
        "# Iterate through the date range\n",
        "for date in pd.date_range(start=start_date, end=end_date, freq='D'):\n",
        "    for minute in range(1440):  # 24 hours * 60 minutes\n",
        "        time = date + timedelta(minutes=minute)\n",
        "\n",
        "        partial_job_counter = random.randint(500, 2500)\n",
        "\n",
        "        # Check if job_counter needs to reset to 2000\n",
        "        if partial_job_counter > job_counter:\n",
        "            job_counter += random.randint(25, 125)\n",
        "\n",
        "        # Check if production_counter needs to reset to 15000\n",
        "        if production_counter > 30000:\n",
        "            production_counter = 15000\n",
        "\n",
        "        # Update production_counter based on job_counter\n",
        "        production_increment = random.randint(125, 375)\n",
        "        production_counter += production_increment\n",
        "\n",
        "        # Update total_counter based on production_counter\n",
        "        total_increment = random.randint(25, 75)\n",
        "        total_counter += total_increment\n",
        "\n",
        "        # Append the data to the extrapolated_data dictionary\n",
        "        extrapolated_data['machineName'].append('GL-LM440-0156')\n",
        "        extrapolated_data['machineStatus'].append(random.choices([2,3,5],weights=[0.48,0.35,0.17])[0])\n",
        "        extrapolated_data['machineType'].append('LM 440')\n",
        "        extrapolated_data['plantName'].append('Pune')\n",
        "        extrapolated_data['smartbearStatus'].append(2)\n",
        "        extrapolated_data['Date'].append(time.strftime('%Y-%m-%d %H:%M'))\n",
        "        extrapolated_data['Job Counter'].append(job_counter)\n",
        "        extrapolated_data['Partial Job Counter'].append(partial_job_counter)\n",
        "        extrapolated_data['Production Counter'].append(production_counter)\n",
        "        extrapolated_data['Total Counter'].append(total_counter)\n",
        "\n",
        "# Create a DataFrame from the extrapolated data\n",
        "extrapolated_df = pd.DataFrame(extrapolated_data)\n",
        "\n",
        "# Save the extrapolated data to a new CSV file\n",
        "extrapolated_df.to_csv('extrapolated_data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PFqPRuFySEfU",
        "sdGeUCc_ofK8"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}